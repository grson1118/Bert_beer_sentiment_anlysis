{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio - ML작업 UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \\* 참고문헌:  \n",
    "> - https://www.gradio.app/docs/interface  \n",
    "> - https://medium.com/@ka.kaewnoparat/text-summarization-and-keyword-extraction-from-customer-reviews-in-french-part-3-3-172195f2816b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ What Does Gradio Do?\n",
    "> One of the best ways to share your machine learning model, API, or data science workflow with others is to create an interactive app that allows your users or colleagues to try out the demo in their browsers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ■ Building demos with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gradio\n",
    "# !pip install -U markupsafe\n",
    "# !pip3 install -U spaCy\n",
    "# !python -m spaCy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# import spacy.cli\n",
    "# spacy.cli.download(\"en_core_web_lg\")\n",
    "# nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "# This Interface class can wrap any Python function with a user interface.\n",
    "# fn: UI 동작원리\n",
    "# inputs: 입력에 어떤 유형을 쓸 것인지 지정 (e.g. \"text\", \"image\" or \"audio\")\n",
    "# outputs: 출력에 어떤 유형을 쓸 것인지 지정 (e.g. \"text\", \"image\" or \"label\")\n",
    "demo = gr.Interface(\n",
    "    fn=greet, \n",
    "    #inputs=\"text\", \n",
    "    inputs = gr.Textbox(lines=2, placeholder=\"여기에 이름을 쓰세요...\"),\n",
    "    outputs=\"text\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name, is_morning, temperature):\n",
    "    salutation = \"Good morning\" if is_morning else \"Good evening\"\n",
    "    greeting = f\"{salutation} {name}. It is {temperature}(F) degrees today\"\n",
    "    celsius = (temperature - 32) * 5 / 9\n",
    "    return greeting, round(celsius, 2)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=greet,\n",
    "    inputs=[\"text\", \"checkbox\", gr.Slider(0, 100)],\n",
    "    outputs=[\"text\", \"number\"],\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "def sepia(input_img):\n",
    "    sepia_filter = np.array([\n",
    "        [0.393, 0.769, 0.189], \n",
    "        [0.349, 0.686, 0.168], \n",
    "        [0.272, 0.534, 0.131]\n",
    "    ])\n",
    "    sepia_img = input_img.dot(sepia_filter.T)\n",
    "    sepia_img /= sepia_img.max()\n",
    "    return sepia_img\n",
    "\n",
    "demo = gr.Interface(sepia, gr.Image(shape=(200, 200)), \"image\")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def greet(name):\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    name = gr.Textbox(label=\"Name\")\n",
    "    output = gr.Textbox(label=\"Output Box\")\n",
    "    greet_btn = gr.Button(\"Greet\")\n",
    "    greet_btn.click(fn=greet, inputs=name, outputs=output, api_name=\"greet\")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Blocks, a low-level API for designing web apps with more flexible layouts and data flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "def flip_text(x):\n",
    "    return x[::-1]\n",
    "\n",
    "\n",
    "def flip_image(x):\n",
    "    return np.fliplr(x)\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"Flip text or image files using this demo.\")\n",
    "    with gr.Tab(\"Flip Text\"):\n",
    "        text_input = gr.Textbox()\n",
    "        text_output = gr.Textbox()\n",
    "        text_button = gr.Button(\"Flip\")\n",
    "    with gr.Tab(\"Flip Image\"):\n",
    "        with gr.Row():\n",
    "            image_input = gr.Image()\n",
    "            image_output = gr.Image()\n",
    "        image_button = gr.Button(\"Flip\")\n",
    "\n",
    "    with gr.Accordion(\"Open for More!\"):\n",
    "        gr.Markdown(\"Look at me...\")\n",
    "\n",
    "    text_button.click(flip_text, inputs=text_input, outputs=text_output)\n",
    "    image_button.click(flip_image, inputs=image_input, outputs=image_output)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ■ 유사 키워드 맵핑 결과 감정별 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gyeom\\OneDrive - 한국항공대학교\\바탕 화면\\머신러닝\\프로젝트\\맥주 측면 감정 분석\\Beer_Sentiment_analysis\\Data\n"
     ]
    }
   ],
   "source": [
    "cd \"./Beer_Sentiment_analysis/Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import spacy\n",
    "import ast\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# 키워드의 초기 빈도 수를 계산\n",
    "def get_initial_counts(keywords):\n",
    "    initial_counts = defaultdict(int)\n",
    "    for keyword in keywords:\n",
    "        initial_counts[keyword] += 1\n",
    "    return initial_counts\n",
    "\n",
    "# 명사 토큰 저장하고 각 유사성을 계산하여 명사 유사성 임계값을 초과하는지 확인\n",
    "# 형용사의 차이는 개인차에 의해 발생하고 명사는 특징 자체를 나타내기에 집중 그룹핑 대상이 됨\n",
    "def custom_similarity(doc1, doc2, noun_similarity_threshold):\n",
    "    doc1_nouns = [token for token in doc1 if token.pos_ == 'NOUN']\n",
    "    doc2_nouns = [token for token in doc2 if token.pos_ == 'NOUN']\n",
    "    noun_similar = any(t1.similarity(t2) > noun_similarity_threshold for t1 in doc1_nouns for t2 in doc2_nouns)\n",
    "    return noun_similar\n",
    "\n",
    "# 유사한 키워드들을 병합하고 키워드 빈도 수와 매핑 저장\n",
    "def merge_keywords(keyword_counts, noun_similarity_threshold):\n",
    "    merged_keywords = defaultdict(int)\n",
    "    keyword_mappings = defaultdict(list)\n",
    "    keyword_docs = {}\n",
    "\n",
    "    # 이미 등록된 키워드는 nlp 계산(임베딩)하지 않고 바로 리턴 (Memoization)\n",
    "    def get_keyword_doc(keyword):\n",
    "        if keyword not in keyword_docs:\n",
    "            keyword_docs[keyword] = nlp(keyword)\n",
    "        return keyword_docs[keyword]\n",
    "\n",
    "    \n",
    "    # 초기 빈도 딕셔너리를 순회하며 각 키워드에 대해 임베딩\n",
    "    for keyword, count in keyword_counts.items():\n",
    "        doc1 = get_keyword_doc(keyword)\n",
    "        merged_or_found_similar = False\n",
    "\n",
    "        # 이미 병합된 키워드 목록에서 해당 키워드와 비교\n",
    "        for merged_keyword in list(merged_keywords):\n",
    "            # 이미 병합된 키워드에 현재 키워드가 포함되어 있다면 빈도를 더함\n",
    "            if keyword == merged_keyword:\n",
    "                merged_keywords[merged_keyword] += count\n",
    "                merged_or_found_similar = True\n",
    "                keyword_mappings[merged_keyword].append(keyword)\n",
    "                break\n",
    "\n",
    "            doc2 = get_keyword_doc(merged_keyword)\n",
    "            \n",
    "            # 유사도가 임계값보다 큰 경우, 두 키워드를 같은 그룹으로 묶음\n",
    "            if custom_similarity(doc1, doc2, noun_similarity_threshold):\n",
    "                merged_keywords[merged_keyword] += count\n",
    "                merged_or_found_similar = True\n",
    "                keyword_mappings[merged_keyword].append(keyword)\n",
    "                break\n",
    "\n",
    "        # 현재 키워드가 기존 병합된 키워드 목록에 포함되지 않고, 유사한 키워드도 없다면 새로운 항목으로 추가\n",
    "        if not merged_or_found_similar:\n",
    "            merged_keywords[keyword] = count\n",
    "            keyword_mappings[keyword] = [keyword]\n",
    "\n",
    "    return merged_keywords, keyword_mappings\n",
    "\n",
    "def merge_similar_keywords_noun_weight(dataframe, noun_similarity_threshold=0.7):\n",
    "    # 문자열로 표현된 리스트를 실제 리스트로 변환\n",
    "    dataframe['Keywords_List'] = dataframe['Keywords'].apply(lambda x: ast.literal_eval(x))\n",
    "    all_keywords = dataframe['Keywords_List'].sum()\n",
    "\n",
    "    # 빈도 수 계산 및 병합\n",
    "    initial_counts = get_initial_counts(all_keywords)\n",
    "    filtered_and_merged_keywords, merged_keyword_mappings = merge_keywords(initial_counts, noun_similarity_threshold)\n",
    "    sorted_merged_keywords = sorted(filtered_and_merged_keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 변환된 'Keywords_List' 컬럼 삭제\n",
    "    dataframe.drop('Keywords_List', axis=1, inplace=True)\n",
    "\n",
    "    return dict(sorted_merged_keywords), dict(merged_keyword_mappings)\n",
    "\n",
    "# filtered_and_merged_keywords, merged_keyword_mappings = merge_similar_keywords(pre_apply_beer_Wired_iStout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 시도1_집계만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gradio as gr\n",
    "\n",
    "pre_apply_beer_Wired_iStout = pd.read_csv('pre_apply_beer_Wired_iStout3_JJ_NN.csv')\n",
    "df_po = pre_apply_beer_Wired_iStout[pre_apply_beer_Wired_iStout['MultinomialNB_label'] == \"Positive\"]\n",
    "df_po.reset_index(drop=True, inplace=True)\n",
    "df_ne = pre_apply_beer_Wired_iStout[pre_apply_beer_Wired_iStout['MultinomialNB_label'] == \"Negative\"]\n",
    "df_ne.reset_index(drop=True, inplace=True)\n",
    "\n",
    "def show_keywords(beer_name, sentiment):\n",
    "    if sentiment == 'Positive': df = df_po\n",
    "    else: df = df_ne\n",
    "\n",
    "    keywords, _ = merge_similar_keywords(df[df['Beer_name'] == beer_name])\n",
    "\n",
    "    return \"\\n\".join([f\"{k}: {v}\" for k, v in keywords.items()])\n",
    "\n",
    "beer_names = pre_apply_beer_Wired_iStout['Beer_name'].unique()\n",
    "sentiments = [\"Positive\", \"Negative\"]\n",
    "\n",
    "interface = gr.Interface(\n",
    "    show_keywords,\n",
    "    [\n",
    "      gr.Dropdown(beer_names, label=\"Beer_name\", info=\"Choose the beer you want to see the summary keyword.\"),\n",
    "      gr.Dropdown(choices=sentiments, label=\"Sentiment\", info=\"Choose Positive or Negative sentiment.\"),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    ")\n",
    "\n",
    "interface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: none; border-top: 2px dashed gray;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - 시도2_집계+매핑 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://35a8b168144390afcc.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://35a8b168144390afcc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gyeom\\AppData\\Local\\Temp\\ipykernel_14516\\2139005315.py:19: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  noun_similar = any(t1.similarity(t2) > noun_similarity_threshold for t1 in doc1_nouns for t2 in doc2_nouns)\n",
      "C:\\Users\\Gyeom\\AppData\\Local\\Temp\\ipykernel_14516\\2139005315.py:19: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  noun_similar = any(t1.similarity(t2) > noun_similarity_threshold for t1 in doc1_nouns for t2 in doc2_nouns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gradio as gr\n",
    "from itertools import islice\n",
    "\n",
    "beer_df = pd.read_csv('pre_apply_beer_Wired_iStout3_JJ_NN.csv')\n",
    "\n",
    "def show_keywords(beer_name, sentiment, flag=0):\n",
    "    one_beer_df = beer_df[beer_df['Beer_name'] == beer_name]\n",
    "    df = one_beer_df[one_beer_df['MultinomialNB_label'] == sentiment]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    keywords, mappings = merge_similar_keywords_noun_weight(df)\n",
    "\n",
    "    if flag == 1:\n",
    "        return \"\\n\".join([f\"{k}: {v}\" for k, v in islice(keywords.items(), None, 10)])\n",
    "    else:\n",
    "        return \"\\n\".join([f\"{k}: {v}\" for k, v in islice(keywords.items(), None, 10)]), mappings\n",
    "\n",
    "def keyword_mappings(beer_name, sentiment, keyword):\n",
    "    _, mappings = show_keywords(beer_name, sentiment)\n",
    "    mapped_keywords = mappings.get(keyword, \"해당 키워드에 매핑된 문자열이 없습니다.\")\n",
    "    return mapped_keywords\n",
    "\n",
    "beer_names = beer_df['Beer_name'].unique()\n",
    "sentiments = [\"Positive\", \"Negative\"]\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            beer_name_dropdown = gr.Dropdown(choices=beer_names, label=\"Beer_name\", info=\"Choose the beer you want to see the summary keyword.\")\n",
    "            sentiment_dropdown = gr.Dropdown(choices=sentiments, label=\"Sentiment\", info=\"Choose Positive or Negative sentiment.\")\n",
    "            keyword_input = gr.Textbox(label=\"Keyword\", info=\"Enter a keyword to see its mappings.\")\n",
    "        with gr.Column():\n",
    "            output_keywords = gr.Textbox(label=\"Summary keywords (Top 10)\")\n",
    "            output_mapping = gr.Textbox(label=\"Keyword mapping\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            review_button = gr.Button(\"Review Topic\")\n",
    "        with gr.Column():\n",
    "            mapping_button = gr.Button(\"Keyword mapping\")\n",
    "\n",
    "    review_button.click(fn=lambda beer_name, sentiment: show_keywords(beer_name, sentiment, flag=1),\n",
    "                    inputs=[beer_name_dropdown, sentiment_dropdown],\n",
    "                    outputs=output_keywords)\n",
    "    mapping_button.click(keyword_mappings,\n",
    "                        inputs=[beer_name_dropdown, sentiment_dropdown, keyword_input],\n",
    "                        outputs=output_mapping)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://416ba780330f1270fd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://416ba780330f1270fd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import pandas as pd\n",
    "import gradio as gra\n",
    "import gradio as gr\n",
    "from itertools import islice\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "import ast\n",
    "\n",
    "# 모델 로드\n",
    "model = BertForSequenceClassification.from_pretrained('/content/drive/MyDrive/sentiment_bert/sentiment_model_BERT_Attention_v2')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "# 토크나이저 초기화\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def analyze_sentiment(sentence):\n",
    "    # 문장을 토크나이징하고 모델 입력으로 변환\n",
    "    inputs = tokenizer(sentence, return_tensors='pt')\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # 모델을 통해 감정 분류 수행\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probabilities = softmax(logits, dim=1)\n",
    "\n",
    "    # 감정 분류 확률 추출\n",
    "    sentiment_labels = ['Negative', 'Positive']\n",
    "    sentiment_probabilities = {label: probability.item() for label, probability in zip(sentiment_labels, probabilities[0])}\n",
    "\n",
    "    return sentiment_probabilities\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    sentiment_probabilities = analyze_sentiment(text)\n",
    "    return sentiment_probabilities\n",
    "\n",
    "###########################################################################################################################################\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# 키워드의 초기 빈도 수를 계산\n",
    "def get_initial_counts(keywords):\n",
    "    initial_counts = defaultdict(int)\n",
    "    for keyword in keywords:\n",
    "        initial_counts[keyword] += 1\n",
    "    return initial_counts\n",
    "\n",
    "# 명사 토큰 저장하고 각 유사성을 계산하여 명사 유사성 임계값을 초과하는지 확인\n",
    "# 형용사의 차이는 개인차에 의해 발생하고 명사는 특징 자체를 나타내기에 집중 그룹핑 대상이 됨\n",
    "def custom_similarity(doc1, doc2, noun_similarity_threshold):\n",
    "    doc1_nouns = [token for token in doc1 if token.pos_ == 'NOUN']\n",
    "    doc2_nouns = [token for token in doc2 if token.pos_ == 'NOUN']\n",
    "    noun_similar = any(t1.similarity(t2) > noun_similarity_threshold for t1 in doc1_nouns for t2 in doc2_nouns)\n",
    "    return noun_similar\n",
    "\n",
    "# 유사한 키워드들을 병합하고 키워드 빈도 수와 매핑 저장\n",
    "def merge_keywords(keyword_counts, noun_similarity_threshold):\n",
    "    merged_keywords = defaultdict(int)\n",
    "    keyword_mappings = defaultdict(list)\n",
    "    keyword_docs = {}\n",
    "\n",
    "    # 이미 등록된 키워드는 nlp 계산(임베딩)하지 않고 바로 리턴 (Memoization)\n",
    "    def get_keyword_doc(keyword):\n",
    "        if keyword not in keyword_docs:\n",
    "            keyword_docs[keyword] = nlp(keyword)\n",
    "        return keyword_docs[keyword]\n",
    "\n",
    "\n",
    "    # 초기 빈도 딕셔너리를 순회하며 각 키워드에 대해 임베딩\n",
    "    for keyword, count in keyword_counts.items():\n",
    "        doc1 = get_keyword_doc(keyword)\n",
    "        merged_or_found_similar = False\n",
    "\n",
    "        # 이미 병합된 키워드 목록에서 해당 키워드와 비교\n",
    "        for merged_keyword in list(merged_keywords):\n",
    "            # 이미 병합된 키워드에 현재 키워드가 포함되어 있다면 빈도를 더함\n",
    "            if keyword == merged_keyword:\n",
    "                merged_keywords[merged_keyword] += count\n",
    "                merged_or_found_similar = True\n",
    "                keyword_mappings[merged_keyword].append(keyword)\n",
    "                break\n",
    "\n",
    "            doc2 = get_keyword_doc(merged_keyword)\n",
    "\n",
    "            # 유사도가 임계값보다 큰 경우, 두 키워드를 같은 그룹으로 묶음\n",
    "            if custom_similarity(doc1, doc2, noun_similarity_threshold):\n",
    "                merged_keywords[merged_keyword] += count\n",
    "                merged_or_found_similar = True\n",
    "                keyword_mappings[merged_keyword].append(keyword)\n",
    "                break\n",
    "\n",
    "        # 현재 키워드가 기존 병합된 키워드 목록에 포함되지 않고, 유사한 키워드도 없다면 새로운 항목으로 추가\n",
    "        if not merged_or_found_similar:\n",
    "            merged_keywords[keyword] = count\n",
    "            keyword_mappings[keyword] = [keyword]\n",
    "\n",
    "    return merged_keywords, keyword_mappings\n",
    "\n",
    "def merge_similar_keywords_noun_weight(dataframe, noun_similarity_threshold=0.7):\n",
    "    # 문자열로 표현된 리스트를 실제 리스트로 변환\n",
    "    dataframe['Keywords_List'] = dataframe['Keywords'].apply(lambda x: ast.literal_eval(x))\n",
    "    all_keywords = dataframe['Keywords_List'].sum()\n",
    "\n",
    "    # 빈도 수 계산 및 병합\n",
    "    initial_counts = get_initial_counts(all_keywords)\n",
    "    filtered_and_merged_keywords, merged_keyword_mappings = merge_keywords(initial_counts, noun_similarity_threshold)\n",
    "    sorted_merged_keywords = sorted(filtered_and_merged_keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 변환된 'Keywords_List' 컬럼 삭제\n",
    "    dataframe.drop('Keywords_List', axis=1, inplace=True)\n",
    "\n",
    "    return dict(sorted_merged_keywords), dict(merged_keyword_mappings)\n",
    "\n",
    "##########################################################################################################################################\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "beer_df = pd.read_csv('pre_apply_beer_Wired_iStout3_JJ_NN.csv')\n",
    "\n",
    "def show_keywords(beer_name, sentiment, flag=0):\n",
    "    one_beer_df = beer_df[beer_df['Beer_name'] == beer_name]\n",
    "    df = one_beer_df[one_beer_df['MultinomialNB_label'] == sentiment]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    keywords, mappings = merge_similar_keywords_noun_weight(df)\n",
    "\n",
    "    if flag == 1:\n",
    "        return \"\\n\".join([f\"{k}: {v}\" for k, v in islice(keywords.items(), None, 10)])\n",
    "    else:\n",
    "        return \"\\n\".join([f\"{k}: {v}\" for k, v in islice(keywords.items(), None, 10)]), mappings\n",
    "\n",
    "def keyword_mappings(beer_name, sentiment, keyword):\n",
    "    _, mappings = show_keywords(beer_name, sentiment)\n",
    "    mapped_keywords = mappings.get(keyword, \"해당 키워드에 매핑된 문자열이 없습니다.\")\n",
    "    return mapped_keywords\n",
    "\n",
    "beer_names = beer_df['Beer_name'].unique()\n",
    "sentiments = [\"Positive\", \"Negative\"]\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            beer_name_dropdown = gr.Dropdown(choices=beer_names, label=\"Beer_name\", info=\"Choose the beer you want to see the summary keyword.\")\n",
    "            sentiment_dropdown = gr.Dropdown(choices=sentiments, label=\"Sentiment\", info=\"Choose Positive or Negative sentiment.\")\n",
    "            keyword_input = gr.Textbox(label=\"Keyword\", info=\"Enter a keyword to see its mappings.\")\n",
    "        with gr.Column():\n",
    "            output_keywords = gr.Textbox(label=\"Summary keywords (Top 10)\")\n",
    "            output_mapping = gr.Textbox(label=\"Keyword mapping\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            review_button = gr.Button(\"Review Topic\")\n",
    "        with gr.Column():\n",
    "            mapping_button = gr.Button(\"Keyword mapping\")\n",
    "\n",
    "    review_button.click(fn=lambda beer_name, sentiment: show_keywords(beer_name, sentiment, flag=1),\n",
    "                    inputs=[beer_name_dropdown, sentiment_dropdown],\n",
    "                    outputs=output_keywords)\n",
    "    mapping_button.click(keyword_mappings,\n",
    "                        inputs=[beer_name_dropdown, sentiment_dropdown, keyword_input],\n",
    "                        outputs=output_mapping)\n",
    "\n",
    "# Tab 1\n",
    "app1 = gra.Interface(fn=sentiment_analysis,\n",
    "                     inputs=\"text\",\n",
    "                     outputs=\"label\",\n",
    "                     live=True,\n",
    "                     title = \"Beer Sentiment Analysis\")\n",
    "\n",
    "# Tab 2\n",
    "app2 = demo\n",
    "\n",
    "# 탭 1과 2를 그룹화\n",
    "tabbed = gra.TabbedInterface([app1,app2],\n",
    "                             [ 'Sentiment Analysis' , 'Keyword Extraction'])\n",
    "tabbed.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ui_python",
   "language": "python",
   "name": "ui_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
